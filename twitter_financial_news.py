# -*- coding: utf-8 -*-
"""TWITTER FINANCIAL NEWS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DzvefJx86txYe9NeFouyGSfEwchpoI9S
"""

from google.colab import files
files.upload()

"""# **LOAD AND CHECK THE DATA**"""

import pandas as pd

# Load both datasets
train_df = pd.read_csv("train_data.csv")
valid_df = pd.read_csv("valid_data.csv")

train_df.head()

valid_df.head()

"""# **CHECK DATASET SIZE AND MISSING VALUES**"""

##Check number of rows and columns

print("Train shape:", train_df.shape)
print("Valid shape:", valid_df.shape)

##Check for missing values

print("Missing values in Train:")
print(train_df.isnull().sum())

print("\nMissing values in Valid:")
print(valid_df.isnull().sum())

"""# **CLEAN AND TWEET TEXT**"""

import re

def clean_text(text):
    text = re.sub(r"http\S+|www.\S+", "", text)         # Remove links
    text = re.sub(r"@\w+|#", "", text)                  # Remove mentions and hashtags
    text = re.sub(r"[^A-Za-z\s]", "", text)             # Remove punctuation and numbers
    text = text.lower()                                 # Convert to lowercase
    return text

train_df['clean_text'] = train_df['text'].apply(clean_text)
valid_df['clean_text'] = valid_df['text'].apply(clean_text)

train_df[['text', 'clean_text']].head()

"""# **LABEL DISTRIBUTION & WORDCLOUD (EDA)**"""

##Check the distribution of labels (How many tweeets per category)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.countplot(x='label', data=train_df, palette='Set3')
plt.title("Tweet Category Distribution in Training Data")
plt.xlabel("Label")
plt.ylabel("Number of Tweets")
plt.show()

from wordcloud import WordCloud

text_all = " ".join(train_df['clean_text'])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_all)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("WordCloud of Training Tweets")
plt.show()

"""# **CONVERT TEXT INTO NUMBERS WITH TWO OPTIONS**"""

##OPTION 1: TF-IDF (for ML Models)

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the vectorizer
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')

# Apply to training and validation text
X_train_tfidf = tfidf.fit_transform(train_df['clean_text'])
X_valid_tfidf = tfidf.transform(valid_df['clean_text'])

# Print the shapes
print("TF-IDF Train Shape:", X_train_tfidf.shape)
print("TF-IDF Valid Shape:", X_valid_tfidf.shape)

"""Explanation 1) TF-IDF = Turns words into numbers based on importance.

2) Words like "stock", "market", "ipo" get weights.

3) Output is a sparse matrix (rows = tweets, cols = words).
"""

##Option 2: Tokenizer + Padding

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')

# Fit on training data
tokenizer.fit_on_texts(train_df['clean_text'])

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(train_df['clean_text'])
X_valid_seq = tokenizer.texts_to_sequences(valid_df['clean_text'])

# Pad sequences to same length
X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post')
X_valid_pad = pad_sequences(X_valid_seq, maxlen=100, padding='post')

# Print shape
print("Padded Train Shape:", X_train_pad.shape)
print("Padded Valid Shape:", X_valid_pad.shape)

"""**Explanation:** 1) Tokenizer = turns words into integers (e.g., "stock" → 21)

2) Padding = makes all tweet sequences the same length (100 words)

3) This format is perfect for Neural Networks

# **MODEL TRAINING - LOGISTIC REGRESSION**
"""

from sklearn.linear_model import LogisticRegression

# Create the model
logreg = LogisticRegression(max_iter=1000)

# Fit on TF-IDF transformed data
logreg.fit(X_train_tfidf, train_df['label'])

"""**Explanation:** 1) max_iter=1000 allows the model to converge even if the data is large.

2) It learns the connection between tweet words (TF-IDF) and their labels (0–19).
"""

##Predict on validation data

y_pred = logreg.predict(X_valid_tfidf)

## Evaluate the model

from sklearn.metrics import classification_report, confusion_matrix

# Print accuracy and report
print("Classification Report:\n")
print(classification_report(valid_df['label'], y_pred))

"""**Explanation:** 1) This shows precision, recall, and f1-score for each label.

2) Helps you see which financial categories your model is doing well on.

# **TRAIN AN ANN(ARTIFICIAL NEURAL NETWORK) MODEL**
"""

##One-Hot Encode the Labels

from tensorflow.keras.utils import to_categorical

y_train_oh = to_categorical(train_df['label'], num_classes=20)
y_valid_oh = to_categorical(valid_df['label'], num_classes=20)

## Build the ANN Model (using TF-IDF features)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# ANN architecture
model_ann = Sequential()
model_ann.add(Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)))
model_ann.add(Dropout(0.3))
model_ann.add(Dense(128, activation='relu'))
model_ann.add(Dropout(0.2))
model_ann.add(Dense(20, activation='softmax'))  # 20 categories

##Compile the Model

model_ann.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

##Fit the Model

history_ann = model_ann.fit(
    X_train_tfidf.toarray(), y_train_oh,
    validation_data=(X_valid_tfidf.toarray(), y_valid_oh),
    epochs=10,
    batch_size=32,
    verbose=1
)

##Plot ANN Accuracy Graph

import matplotlib.pyplot as plt

plt.plot(history_ann.history['accuracy'], label='Training Accuracy', color='teal')
plt.plot(history_ann.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title("ANN Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

##Confusion Matrix

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Predict and decode
y_pred_ann = np.argmax(model_ann.predict(X_valid_tfidf.toarray()), axis=1)
y_true_ann = np.argmax(y_valid_oh, axis=1)

# Confusion matrix
cm_ann = confusion_matrix(y_true_ann, y_pred_ann)

# Plot
plt.figure(figsize=(12,8))
sns.heatmap(cm_ann, annot=True, fmt='d', cmap='crest')
plt.title("Confusion Matrix - ANN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **CNN Model for Text Classification**"""

##Build the CNN Model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout

cnn_model = Sequential()
cnn_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
cnn_model.add(GlobalMaxPooling1D())
cnn_model.add(Dense(64, activation='relu'))
cnn_model.add(Dropout(0.3))
cnn_model.add(Dense(20, activation='softmax'))  # 20 categories

##Compile the Model

cnn_model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

##Train the Model

cnn_history = cnn_model.fit(
    X_train_pad, y_train_oh,
    validation_data=(X_valid_pad, y_valid_oh),
    epochs=6,
    batch_size=32,
    verbose=1
)

##Visualize Accuracy Over Epochs

import matplotlib.pyplot as plt

plt.plot(cnn_history.history['accuracy'], label='Train Acc', color='purple')
plt.plot(cnn_history.history['val_accuracy'], label='Val Acc', color='gold')
plt.title("CNN Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

##Confusion Matrix for CNN

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Predictions
y_pred_cnn = np.argmax(cnn_model.predict(X_valid_pad), axis=1)
y_true_cnn = np.argmax(y_valid_oh, axis=1)

# Confusion Matrix
cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)

plt.figure(figsize=(12,8))
sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='YlOrBr')
plt.title("Confusion Matrix - CNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **RNN (RECURRENT NEURAL NETWORK)**"""

##Build a Simple RNN Model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout

rnn_model = Sequential()
rnn_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
rnn_model.add(SimpleRNN(128, return_sequences=False))
rnn_model.add(Dropout(0.3))
rnn_model.add(Dense(64, activation='relu'))
rnn_model.add(Dense(20, activation='softmax'))  # 20 categories

##Compile the Model

rnn_model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

##Train the Model

rnn_history = rnn_model.fit(
    X_train_pad, y_train_oh,
    validation_data=(X_valid_pad, y_valid_oh),
    epochs=5,
    batch_size=32,
    verbose=1
)

##Accuracy Graph

import matplotlib.pyplot as plt

plt.plot(rnn_history.history['accuracy'], label='Train Acc', color='green')
plt.plot(rnn_history.history['val_accuracy'], label='Val Acc', color='orange')
plt.title("RNN Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

##Confusion Matrix

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred_rnn = np.argmax(rnn_model.predict(X_valid_pad), axis=1)
y_true_rnn = np.argmax(y_valid_oh, axis=1)

cm_rnn = confusion_matrix(y_true_rnn, y_pred_rnn)

plt.figure(figsize=(12,8))
sns.heatmap(cm_rnn, annot=True, fmt='d', cmap='coolwarm')
plt.title("Confusion Matrix - RNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **MODEL COMPARISON**"""

# Extract final accuracy values
log_acc = ...                 # Logistic Regression accuracy (manual)
ann_acc = history_ann.history['val_accuracy'][-1]
cnn_acc = cnn_history.history['val_accuracy'][-1]
rnn_acc = rnn_history.history['val_accuracy'][-1]

from sklearn.metrics import accuracy_score

log_acc = accuracy_score(valid_df['label'], y_pred)

##Create Comparison Table

# Create a dictionary of model performances
model_results = {
    'Model': ['Logistic Regression', 'ANN', 'CNN', 'RNN'],
    'Validation Accuracy': [log_acc, ann_acc, cnn_acc, rnn_acc]
}

# Convert to DataFrame
results_df = pd.DataFrame(model_results)
results_df.sort_values(by='Validation Accuracy', ascending=False, inplace=True)
results_df

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.barplot(x='Validation Accuracy', y='Model', data=results_df, palette='viridis')
plt.title("Model Comparison - Validation Accuracy")
plt.xlim(0.0, 1.0)
plt.show()

"""# **CONCLUSION**

This project focused on building a multi-label classification system to categorize finance-related tweets into 20 specific topics such as IPOs, Stock Commentary, Macro Trends, and more. The dataset underwent thorough preprocessing, including cleaning, tokenization, and vectorization to prepare it for modeling.

Various machine learning and deep learning models were implemented, including Logistic Regression, Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). Each model was evaluated using metrics like accuracy, classification report, and confusion matrix.

Among all models, [insert your best-performing model here] delivered the highest validation accuracy and consistent predictions across categories. The model comparison helped highlight how different algorithms perform in multi-label text classification tasks.

In conclusion, this project demonstrates the power of NLP and AI techniques in analyzing financial text data, enabling smarter content categorization, better information filtering, and potential use in finance-related news tracking tools.


"""